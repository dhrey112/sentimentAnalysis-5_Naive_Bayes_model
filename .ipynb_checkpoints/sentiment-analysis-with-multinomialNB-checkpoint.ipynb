{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### autoreload modules and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import all neceesary libraries/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary utilities\n",
    "\n",
    "The utilities file is for cleaning of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utilities\n",
    "from utils import clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the data\n",
    "\n",
    "We will be using combination of of imdb, yelp and amazon cells users' review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb = pd.read_csv('imdb_labelled.txt', sep='\\t', names=[ 'review', 'label'])\n",
    "\n",
    "df_yelp = pd.read_csv('yelp_labelled.txt', sep='\\t', names=[ 'review', 'label'])\n",
    "\n",
    "df_amzn = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', names=[ 'review', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the shape for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb.shape, df_yelp.shape, df_amzn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We join the dataset together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df_imdb.append([df_yelp, df_amzn], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checeking the data shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Label (Positive/Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['sentiment'] = data[\"label\"].apply(lambda x: \"positive\" if x else \"negative\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We check for the sum of missing values if there exist any, fortunately there is no missing value in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index = [1, 0]\n",
    "\n",
    "print(data[\"sentiment\"].value_counts())\n",
    "print()\n",
    "\n",
    "barlist = plt.bar(Index, data[\"sentiment\"].value_counts())\n",
    "\n",
    "plt.title(\"Frequency of Sentiments\")\n",
    "plt.xticks(Index, ['positive', 'negative'])\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xlabel('Sentiment expressed in Reviews')\n",
    "\n",
    "barlist[Index[1]].set_color('green')\n",
    "barlist[Index[0]].set_color('red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataset labels seems to be balanced, there will be no need for any under/over sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of Data\n",
    "This is done by using utilities function(clean_text) as .apply parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_review'] = data[\"review\"].apply(clean_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of Review\n",
    "\n",
    "We will be visualising the review using wordcloud module. The module visualised the most frequent words in large size and the less frequent words in smaller sizes.\n",
    "\n",
    "#### Objective:\n",
    "    1. To know the most frequent words\n",
    "    2. Words that contributed to Negative stress\n",
    "    3. Words that contributed to Positive stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ''.join([text for text in data['clean_review']])\n",
    "\n",
    "cloud_word = WordCloud(width= 600, height = 500, random_state = 31, max_font_size = 100, background_color='white').generate(words)\n",
    "\n",
    "plt.figure(figsize = (10, 15))\n",
    "plt.imshow(cloud_word, interpolation = \"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see most of the words are positive or neutral. With film, movie, good, character being the most frequent ones.  Similarly, we will plot the word cloud for the Positive and Negative score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = ''.join([text for text in data['clean_review'][data['label']==1]])\n",
    "\n",
    "cloud_word = WordCloud(width= 600, height = 500, max_font_size = 100, background_color='green').generate(positive_words)\n",
    "\n",
    "plt.figure(figsize = (10, 25))\n",
    "plt.imshow(cloud_word, interpolation = \"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = ''.join([text for text in data['clean_review'][data['label']==0]])\n",
    "\n",
    "cloud_word = WordCloud(width= 600, height = 500, max_font_size = 100, background_color='black').generate(negative_words)\n",
    "\n",
    "plt.figure(figsize = (10, 15))\n",
    "plt.imshow(cloud_word, interpolation = \"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that it was saturated by common words like film, watch, very etc; there are nagative words like bore, suck, redicule etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "Observe count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,1),lowercase=True, stop_words='english')\n",
    "count_data = count_vectorizer.fit_transform(data[\"clean_review\"])\n",
    "cv_dataframe = pd.DataFrame(count_data.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "cv_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec = TfidfVectorizer(use_idf=True, smooth_idf=True, ngram_range=(1,1),stop_words='english')\n",
    "\n",
    "tf_idf_data = tf_idf_vec.fit_transform(data[\"clean_review\"])\n",
    "tf_idf_dataframe = pd.DataFrame(tf_idf_data.toarray(), columns=tf_idf_vec.get_feature_names())\n",
    "tf_idf_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What CountVectorizer and Vectorizers do is to split the sentences into words, count the number of occurences of each word which is called bag of word model, the higher the frequency of a word really means its not so important or close to irrelevance, therefore the inverse logarithm is been carried out, with the result, you can easily deduce that the closer the value is towards 0 the less relevant it is. They are a very useful preprocessing tool used for text classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['clean_review']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This is where we split the dataset in preparation for the training using a sklearn module named train_test_split, reviews ia what we are using to make predictions therefore it is our feature in this case and scores of the reviews is our target variable, what we are predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "print(f\"train shape input:{X_train.shape}, output:{y_train.shape}\")\n",
    "print(f\"test shape  input:{X_test.shape}, output:{y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with TfidfVector and Bernoulli NB\n",
    "\n",
    "\n",
    "#### initilize pipeline with TfidfVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeNB = Pipeline([\n",
    "   # ('tfidf',TfidfVectorizer(analyzer=clean_text, stop_words=\"english\")),\n",
    "      ('tfidf',TfidfVectorizer()),\n",
    "      ('classifier',BernoulliNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeNB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipeNB, \"models/bernoulli_naive_bayes_with_tfidf_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeNB.predict(X_test) #predict testing data\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Cross validation with TfidfvVectorizer and MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=100) \n",
    "skf.get_n_splits(X, y) \n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index] \n",
    "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    pipeNB.fit(x_train_fold, y_train_fold)\n",
    "    result = pipeNB.score(x_test_fold, y_test_fold)\n",
    "    accuracy.append(result)\n",
    "\n",
    "accuracy = np.array(accuracy)\n",
    "\n",
    "# Print the output\n",
    "print('List of first 10 possible accuracy:')\n",
    "for index, acc in enumerate(accuracy[:10]):\n",
    "    print(f\"{index+1:3d}. {acc:.4f}\")\n",
    "\n",
    "print('\\nMetrics that were obtained from this model:')\n",
    "print(f' Maximum Accuracy:   {accuracy.max()*100:.2f}%') \n",
    "print(f' Minimum Accuracy:   {accuracy.min()*100:.2f}%') \n",
    "print(f' Mean Accuracy:   {accuracy.mean()*100:.2f}%') \n",
    "print(f' Standard Deviation: {accuracy.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Cross validation with Countvectorizer and MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeNB = Pipeline([\n",
    "    #       ('bow', CountVectorizer(analyzer=clean_text, stop_words=\"english\")),\n",
    "      ('bow',CountVectorizer()),\n",
    "      ('classifier',BernoulliNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeNB.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipeNB, \"models/bernoulli_naive_bayes_with_count_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = pipeNB.predict(X_test) #predict testing data\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Cross validation with Countvectorizer and MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=100) \n",
    "skf.get_n_splits(X,y) \n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index] \n",
    "  y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "  pipeNB.fit(X_train_fold, y_train_fold)\n",
    "  result = pipeNB.score(X_test_fold, y_test_fold)\n",
    "  accuracy.append(result)\n",
    "\n",
    "\n",
    "accuracy = np.array(accuracy)\n",
    "\n",
    "# Print the output\n",
    "print('List of first 10 possible accuracy:')\n",
    "for index, acc in enumerate(accuracy[:10]):\n",
    "    print(f\"{index+1:3d}. {acc:.4f}\")\n",
    "\n",
    "print('\\nMetrics that were obtained from this model:')\n",
    "print(f' Maximum Accuracy:   {accuracy.max()*100:.2f}%') \n",
    "print(f' Minimum Accuracy:   {accuracy.min()*100:.2f}%') \n",
    "print(f' Mean Accuracy:   {accuracy.mean()*100:.2f}%') \n",
    "print(f' Standard Deviation: {accuracy.std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
